# Homework 5

*This week we learnt about dimensionality reduction techniques.*

```{r}
date()
```

```{r}

#Loading the data
human <- read.table("~/IODS-project/data/human.txt", sep="\t")

```

# 1) Overview of the data

Below is a graphical overview of the data, and summaries of the variables. 

```{r}

# Access GGally
library(GGally)


library(tidyr)
library(corrplot)

# visualize the 'human' variables
ggpairs(human)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)

# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)


```

__Description:__ 

Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-3 points)


# 2) Principal component analysis (PCA)

This will be performed on the not standardized data. Show the variability captured by the principal components. 

Draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables. (0-2 points)


```{r}

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)

# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.8,1), col = c("grey40", "deeppink2"))

```

__Description:__ Draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables. (0-2 points)

```{r}

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

__Description:__ The variable crime is now a factor variable with levels low, med_low, med_high and high.

Next, I will divide the dataset to training and test sets, so that 80% of the data belongs to the train set.

```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create training set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

# 3) Linear Discriminant Analysis


Standardize the variables in the human data and repeat the above analysis. Interpret the results of both analysis (with and without standardizing). Are the results different? Why or why not? Include captions (brief descriptions) in your plots where you describe the results by using not just your variable names, but the actual phenomenons they relate to. (0-4 points)

Give your personal interpretations of the first two principal component dimensions based on the biplot drawn after PCA on the standardized human data. (0-2 points)

Load the tea dataset from the package Factominer. Explore the data briefly: look at the structure and the dimensions of the data and visualize it. Then do Multiple Correspondence Analysis on the tea data (or to a certain columns of the data, itâ€™s up to you). Interpret the results of the MCA and draw at least the variable biplot of the analysis. You can also explore other plotting options for MCA. Comment on the output of the plots. (0-4 points)




I will fit the linear discriminant analysis on the train set created above, using the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables.

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

Next, I will predict the crime classes with the LDA model on the test data. 

```{r}

# first, save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# then, predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

__Description:__ The LDA model does quite a good job at predicting the crime classes on the test data. When the correct class is low, the model predicts mostly correctly. When the correct class is med_low, most predictions are still correct, but now there are predictions in three different categories. When the correct class is med_high, again the model predicts mostly correctly. When the class is high, the model predicts all cases correctly. Therefore, the model is at its best when predicting the high crime rate class.

# 4) k-means clustering

Next, I will reload the Boston dataset, standardize it and then calculate the distances between observations. This is a step towards using a k-means algorithm on the dataset.

```{r}

# load the data
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# Calculating distances below
# euclidean distance matrix, default option
dist_eu <- dist(boston_scaled)

# look at the summary of the distances, first with the default option
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")

# look at the summary of the distances, now with the manhattan method
summary(dist_man)


```

Next, I will run a k-means algorithm on the dataset. 

```{r}

# k-means clustering, first with 10 clusters
km <-kmeans(boston_scaled, centers = 10)

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

__Description:__ Looks like K = 2 is the optimal number of clusters here. Using that, I'm running the algorithm again (below).

```{r}

# k-means clustering, with two clusters
km <-kmeans(boston_scaled, centers = 2)

# plot the dataset with the clusters
pairs(boston_scaled, col = km$cluster)

```

__Description:__ The whole dataset is plotted above, with the two clusters illustrated by different colours. Because the small plots are hard to see, I will split the variables to smaller groups (below) for easier viewing.

```{r}

# plot the dataset with the clusters
pairs(boston_scaled[1:5], col = km$cluster)

# plot the dataset with the clusters
pairs(boston_scaled[6:10], col = km$cluster)

# plot the dataset with the clusters
pairs(boston_scaled[11:14], col = km$cluster)

```