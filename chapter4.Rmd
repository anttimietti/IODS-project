# Homework 4

*This week we learnt about clustering & classification.*

```{r}
date()
```

# 1) Overview of the data

Exploring the structure and the dimensions of the data:

```{r}

# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
dim(Boston)

```

__Description:__ The dataset is a data frame with 506 rows (observations) and 14 columns (variables). All variables in the dataset are numerical or integers. The dataset describes housing values in suburbs of Boston. More information about the data can be found here: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

Below is a graphical overview of the data, and summaries of the variables in it. 

```{r}

library(tidyverse)
library(corrplot)

# if you want to plot a matrix of the variables, use the bit below
#pairs(Boston)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

# summarize the variables
summary(Boston)

```
__Description:__ The correlation plot and matrix above show that e.g. age (proportion of owner-occupied units built prior to 1940) and dis (weighted mean of distances to five Boston employment centres), nox (nitrogen oxides concentration) and indus (proportion of non-retail business acres per town) are strongly negatively correlated. On the other hand, crim (per capita crime rate by town) is positively correlated with rad (index of accessibility to radial highways) and tax (full-value property-tax rate). Some not-so-surprising relationships can be seen, such as a strong negative correlation between medv (median value of owner-occupied homes) and lstat (lower status of the population as %).

The summary statistics show that some variables have a wide distribution (e.g. tax), while most observations of some other variables are found on a much narrower range (e.g. chas).

# 2) Preparing the data for Linear Discriminant Analysis

First, I will standardize the dataset and print out summaries of the scaled data (below). 

```{r}

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```

__Description:__ The dataset is now scaled by subtracting the column means from the corresponding columns and dividing the difference with standard deviation. The values of the variables are now standardized, and we can see their values are now much closer to each other than with the unscaled dataset.

Next, I will create a categorical variable of the scaled crime rate in the dataset, using the quantiles as break points in the categorical variable. 

```{r}

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

__Description:__ The variable crime is now a factor variable with levels low, med_low, med_high and high.

Next, I will divide the dataset to training and test sets, so that 80% of the data belongs to the train set.

```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create training set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

# 3) Linear Discriminant Analysis

I will fit the linear discriminant analysis on the train set created above, using the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables.

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
# I hope this shows in the diary file, as the two lines of code above should be run together

```
Next, I will predict the crime classes with the LDA model on the test data. 

```{r}

# first, save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# then, predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)

__Description:__ The LDA model does quite a good job at predicting the crime classes on the test data. When the correct class is low, the model predicts mostly correctly. When the correct class is med_low, most predictions are still correct, but now there are predictions in three different categories. When the correct class is med_high, again the model predicts mostly correctly. When the class is high, the model predicts all cases correctly. Therefore, the model is at its best when predicting the high crime rate class.

# 4) k-means clustering

Next, I will reload the Boston dataset, standardize it and then calculate the distances between observations. This is a step towards using a k-means algorithm on the dataset.

```{r}

# load the data
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# Calculating distances below
# euclidean distance matrix, default option
dist_eu <- dist(boston_scaled)

# look at the summary of the distances, first with the default option
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")

# look at the summary of the distances, now with the manhattan method
summary(dist_man)


```

Next, I will run a k-means algorithm on the dataset. 

```{r}

# k-means clustering, first with 10 clusters
km <-kmeans(boston_scaled, centers = 10)

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
__Description:__ Looks like K = 2 is the optimal number of clusters here. Using that, I'm running the algorithm again (below).

```{r}

# k-means clustering, with two clusters
km <-kmeans(boston_scaled, centers = 2)

# plot the dataset with the clusters
pairs(boston_scaled, col = km$cluster)

```
__Description:__ The whole dataset is plotted above, with the two clusters illustrated by different colours.